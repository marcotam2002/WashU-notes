\documentclass[12pt]{article}
% We can write notes using the percent symbol!
% The first line above is to announce we are beginning a document, an article in this case, and we want the default font size to be 12pt
\usepackage[utf8]{inputenc}
% This is a package to accept utf8 input.  I normally do not use it in my documents, but it was here by default in Overleaf.
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
% These three packages are from the American Mathematical Society and includes all of the important symbols and operations 
\usepackage{fullpage}
% By default, an article has some vary large margins to fit the smaller page format.  This allows us to use more standard margins.

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{remark}{Remark}
\newtheorem{terminology}{Terminology}
\newtheorem{example}{Example}

\setlength{\parskip}{0em}
% This gives us a full line break when we write a new paragraph

\begin{document}
% Once we have all of our packages and setting announced, we need to begin our document.  You will notice that at the end of the writing there is an end document statements.  Many options use this begin and end syntax.

\begin{center}
    \Large MATH495 Stochastic Process Note
\end{center}

\section{Background}

During my exchange in WashU, I've typed some of my notes from lectures and collaborated some with online resources and my understanding. This is mainly for the course - Stochastic Process, as the professor doesn't have a typed note. Since I typed the notes quite rush, there maybe some mistakes or typos. I've typed the note mainly for my revision and probably future review. But I am also glad if this note can help anyone need it.

\section{CTMC Basic Properties} 

\begin{definition}
     A continuous time process $\{X_t\}_{t \geq 0}$ taking values in a countable set S is said to be a (continuous-time) markov chain if, for any $s_0 < s_1 < \cdots < s_n < s < t$ and $i ,j, i_0, \cdots , i_n$ we have $P[X_{t+s} = j | X_s = i, X_{s_n} = i_n, \cdots , X_{s_0} = i_0] = P[X_t = j | X_0 = i]$
\end{definition}

\begin{terminology}
    $p_{ij}(t) = P[X_t = j | X_0 = i]$ are called \textbf{transition probabilities}. This means the starting state is i when time = 0 and going to j when time = t. 
    \\
    \\The matrix $P(t) = \begin{bmatrix}
 p_{11}(t) & \cdots & p_{1N}(t)\\
 \vdots  &  & \vdots \\
 p_{N1}(t) & \cdots & p_{NN}(t)
\end{bmatrix}$ is called the transition probability matrix of the chain.
\end{terminology}

\begin{example}
 A homogeneous Poisson process $\{N_t\}_{t\geq0}$ is a markov process with state space S = $\{0, 1, 2, \cdots\}$.
\end{example}

\begin{theorem}
    Suppose $\{Y_n\}_{n \geq 0}$ is a discrete-time markov chain and $\{N_t\}_{t\geq0}$ is an independent homogeneous Poisson process. Then, $X_t = Y_{N_t}$ is a markov process.
\end{theorem}

\subsection{Matrix exponential}

\begin{lemma}
    $e^{A} = \sum_{k = 0}^{\infty } \frac{1}{k!}A^k $ where $A^0 = I$
    \begin{enumerate}
        \item $e^0 = I$ where 0 is the zero matrix
        \item $e^{cI} = e^cI$ for some scalar c
        \item $e^{UAU^{-1}} = Ue^AU^{-1}$
        \item $AB = BA \Rightarrow e^Ae^B = e^{A+B}$
    \end{enumerate}
\end{lemma}

\subsection{Fundamental Construction of CTMC}

One very large class of CTMC (that cover almost all cases) is based on independent exponential times. The parameters or inputs of the process are some nonnegative constants $q(i,j), i, j \in S$ with $i \ne j$. which are called \textbf{jump rates}. (as we shall see $q(i,j)$ is the rate at which the chain jump from i to j).
\\
\\Next, set
\begin{center}
$\lambda _{i} = \sum_{j \ne i} q(i,j)$

$r_{i,j} = \frac{q(i,j)}{\lambda_i}$
\end{center}
where $\lambda_i$ is the rate at which the chain leaves i and $r(i,j)$ is the probability of going to j when leaving i.

\subsection{Idea of the Construction Procedure}

The previous parameters determine the dynamics of the chain in a simple way. If the chain $X$ arrives to a state i such that $\lambda_i = 0$, then it would stay there forever. But if $\lambda_i > 0$ then the chain will stay there an exponential time d with rate $\lambda$. The chain with then decide to jump to another state $j \ne i$ with probability $r(i,j)$.
\\
\\Equivalently, we can explicitly write $\{X_t\}$ in terms of a discrete time markov chain $\{Y_n\}_{n\geq0}$ with transition probabilities $r(i,j)$ and a sequence of i.i.d exponential ($\lambda = 1$) time $\tau _0, \tau _1, \cdots$ as follows:

\begin{center}
    $X(t) = Y_n$ if $T_n \leq t < T_{n+1}$
\end{center}

where $T_n = t_1 + \cdots + t_n$ with $t_i = \frac{\tau_{i-1}}{\lambda(Y_{i-1})} \sim exp(\lambda(Y_{i-1}))$

\begin{terminology}
    The discrete-time process $\{Y_n\}_{n\geq0}$ that records the consecutive states of the chain is called the \textbf{embedded markov chain} and it can be proved that it is indeed a markov chain.
    \\
    \\The rates $q(i,j)$ are typically arrange in the following matrix form
    \begin{center}
        $Q = \begin{bmatrix}
         -\lambda _1 & q(1,2) & q(1,3) & \cdots & q(1,N)\\
         q(2,1) & -\lambda _2 & q(2,3) & \cdots & q(2,N)\\
         \vdots &  &  & & \vdots \\
         q(N,1) & q(N,2) & q(N,3) & \cdots & -\lambda _N\\
        \end{bmatrix}$
    \end{center}
    This matrix is called \textbf{the infinitesimal generator} of the process.
    \\
    \\It is customary to draw the chain as a connected graph with indices representing states and arrows connecting states $i \ne j$ such that $q(i,j) > 0$. Specifically, an arrow goes from i to j if $q(i,j) > 0$. The arrow is labeled by $q(i.j)$. This graph is sometimes called \textbf{transition rate graph}.
\end{terminology}

\subsection{Interpretation of $q(i,j)$}

As it turns out, $\lim_{h  \to 0} \frac{P[X_{t+h}|X_t=i]}{h}=q(i,j), i\ne j$ or equivalently, $P[X_{t+h}=j|X_t=i]=q(i,j)h+\theta(h)$. The second definition is the probability that a chain in state i jumps to another state j in a small time interval h.
\\
\\We can see from the definition $q(i,j)$ means the probability goes from state i to state j in a very short time. Therefore, we say this probability as jump rate.

\subsection{Limiting distribution}

The following theorem formalizes the face that the chain describes in the construction procedure is indeed a CTMC:

\begin{theorem}
    Let $\{X_t\}t>0$ be the process. Then $\{X_t\}$ is CTMC with transition probability
    \begin{center}
        $P(t) = \begin{bmatrix}
         P_{11}(t) & \cdots & P_{1N}(t) \\
         \vdots &  & \vdots \\
         P_{N1}(t) & \cdots & P_{NN}(t)
        \end{bmatrix} = e^{tQ}$
    \end{center}
\end{theorem}

\begin{lemma}
    In particular, if Q has the eigendecomposition: $Q = U\Lambda U^{-1}$ where $\Lambda = diag(\alpha_1, \cdots, \alpha_N)$ is the diagonal matrix of eigenvalues, then $P(t) = e^{tQ} = Ue^{t\Lambda}U^{-1}$, where $e^{t\Lambda}=diag(e^{t\alpha_1}, \cdots, e^{t\alpha_N})$.
\end{lemma}

\begin{lemma}
    Note that $\alpha = 0$ is always an eigenvalue for Q corresponding to the eigenvector $[1,1, \cdots, 1]$. Moreover, note that $\lim_{t \to \infty}P(t)$ exists iff all $\alpha_i \leq 0$.
\end{lemma}

\begin{example}
    $Q = \begin{bmatrix}
     -1 & 1\\
     2 & -2
    \end{bmatrix}$ Find its transition probabilities and determine its long term limiting distribution, if it exists, $\lim_{t \to \infty}P[X_t = j | X_0 = i] = \pi_j$.

    \begin{enumerate}
        \item We first diagonalize Q. Note that we have $\lambda = 0$ or $\lambda = -3$.
        \item For $\lambda = 0$, the eigenvector $= [1, 1]^T$. For $\lambda = -3$, the eigenvector $= [1, -2]^T$.
        \item Compute $P_t = Ue^{tQ}U^{-1} = \begin{bmatrix}
         \frac{2}{3}+\frac{1}{3}e^{-3t} & \frac{1}{3}-\frac{1}{3}e^{-3t}\\
         \frac{2}{3}-\frac{2}{3}e^{-3t} & \frac{1}{3}+\frac{2}{3}e^{-3t}
        \end{bmatrix}$
        \item $\lim_{t \to \infty}P(t) = \begin{bmatrix}
         \frac{2}{3} & \frac{1}{3}\\
         \frac{2}{3} & \frac{1}{3}
        \end{bmatrix}$ so $\pi = [\frac{2}{3}, \frac{1}{3}]$
    \end{enumerate}
\end{example}

\begin{remark}
    In the example above, $[\frac{2}{3}, \frac{1}{3}]$ is the limiting distribution. This coincides with the row of $U^{-1}$ corresponding to the eigenvalue $\alpha = 0$. In that example, the top row of $U^{-1}$. The limit $\lim_{t \to \infty}P[X_t = i]$ exists whenever $\alpha_i \leq 0, \forall i$.
    \\
    \\But, in order for this not to depend on the initial distribution, we must have that 0 is a simple eigenvalue and all other $\alpha_i$ must be negative.
\end{remark}

\section{CTMC Absorption times Probability}

For a discrete-time MC< we consider the following problems:
\begin{enumerate}
    \item \textbf{Absorption Times Problems}\\
    \\ The average time it takes for the chain to be absorbed by one of the recurrent classes when starting at a transient state.
    \item \textbf{Absorption Probability Problems}\\
    \\ What is the probability that when starting at some state i, the chain will be absorbed by a specific recurrent class instead of others?
\end{enumerate}
In this section, we will show how to answer this kind of questions for a CTMC.

\subsection{Solution procedure for problem 1}

For the absorption times problem, suppose A consists of all recurrent states and $T_r = S \setminus A$ are the transient states. We want to compute:
\begin{center}
    $g(i) = E_i[T_A]$ where $T_A = inf\{t\geq0: X_t \in A\}$
\end{center}
In this case, we have $g = \begin{bmatrix}
 g(i_1) \\
\vdots \\
g(i_m) \\
\end{bmatrix} = (-Q^{sub})\begin{bmatrix}
 1\\
 \vdots\\
1
\end{bmatrix}$
where $Q^{sub}$ is the submatrix of Q corresponding to the transient states.

\begin{example}
    A shop has two barbers that can cut hair at rate 3 people per hour. Customers arrive at times of a rate 2 Poisson process, but will leave if there are two people getting their haircut and two waiting. The state of the system to be the number of people in the shop. Find $P_i[V_0 < V_4]$ for $i = 1,2,3$.
    \\
    \\(a) Derive the infinitesimal generator Q.
    \\(b) Compute $E[T_0|X_0=x]$, $x = 1,2,3,4$, where $T_0 = inf\{t>0: X_t=0\}$.
    \\
    \\We can find the transition rate graph first with $q(i,i+1) = 2$ for $i = 0,1,2,3$, $q(i,i-1)=6$ for $i = 2,3,4$ and $q(1,0) = 3$. $q(i,i+1) = 2$ because customers arrive at times of a rate 2. We have two barbers so the shop can have 6 people hair cut per hour. Therefore, when we have more than 1 customer, $q(i,i-1)=6$. While if we only have 1 customer, then it is not related to the number of the barbers so $q(1,0) = 3$.
    \\
    \\Hence, we can write $Q = \begin{bmatrix}
         -2 & 2 & 0 & 0 & 0 \\
         3 & -5 & 2 & 0 & 0\\
         0 & 6 & -8 & 2 & 0\\
         0 & 0 & 6 & -8 & 2\\
         0 & 0 & 0 & 6 & -6
        \end{bmatrix}$.
    \\
    \\Note that $q(0,1)=2$ means the rate of arrival of new customers; $q(1,0)=3$ means the rate at which 1 customer is served by the barber; $q(2,1)=6$ means the rate at which the two customers will be served.
    \\
    \\Suppose $X_0 = 1$ (only one customer in the shop). Let $D_1$ be the time to serve the customer $\sim exp(\lambda=3)$ and $A_1$ be the arrival time of a new customer $\sim exp(\lambda=2)$. Then, the time the chain will spend in time 0 is $T_0 = min\{D_1,A_1\} \sim(3+2) \Rightarrow \lambda_0 = $ holding rate at time 0 $= 5$. After $T_0$, the chain will jump to either state 0 or state 2 with probabilities $r(1,0) = P[D_1 < A_1] = \frac{3}{5}$ and $r(1,0) = P[D_1 > A_1] = \frac{2}{5}$. Recalling that $r(i,j) = \frac{q(i,j)}{\lambda_i}$, we get the rates $q(1,0) = \frac{3}{5}\times5=3$ and $q(1,2) = \frac{2}{5}\times5=2$
    \\
    \\Similarly, if $X_0 = 2$, the time X spends in time 2 is $T_2=min\{D_1,D_2,A_1\} \sim exp(3+3+2)$. Then $\lambda_2 = 8$. After $T_2$, the chain jumps to state 1 or 3 with probs: $r(2,1) = P(min\{D_1,D_2\}<A_1) = \frac{6}{8}$ and $r(2,3) = P(min\{D_1,D_2\}>A_1) = \frac{2}{8}$. Then, we get the rates $q(2,1) = \frac{6}{8}\times8=6$ and $q(2,3) = \frac{2}{8}\times8=2$.
    \\
    \\Now, we discuss part b. The idea is to think of state 0 as an absorbing state and then follow the procedure.
    \begin{enumerate}
        \item Extract the matrix $Q^{sub}$.\\
        \\$Q^{sub} = \begin{bmatrix}
         -5 & 2 & 0 & 0\\
         6 & -8 & 2 & 0\\
         0 & 6 & -8 & 2\\
         0 & 0 & 6 & -6
        \end{bmatrix}$.
        \item Compute $(-Q^{sub})^{-1}$
        \item Multiply by $(1,1,1,1)^T$
    \end{enumerate}
    Hence, we get $g(i) = (\frac{40}{81}, \frac{119}{162}, \frac{155}{162},  \frac{91}{81})$ where $E[T_0|X_0=0] = \frac{40}{81}$.
\end{example}

\subsection{Solution procedure for problem 2}

For absorption probability, it should be clear that we can consider the analogous question for the embedded discrete-time chain $\{Y_n\}_{n\geq0}$ with transition probabilities $r(i,j)=\frac{q(i,j)}{\lambda_i}$.
\\
\\But we can also use directly Q. Concretely, suppose we have 2 closed recurrent classes $R_1$ and $R_2$ and the rest are transient $T_r = S \setminus (R_1 \cup R_2) = {i_1, \cdots, i_m}$.
\\
\\We want to compute $l(i) = P_i[T_{R_1} < T_{R_2}]$, $i \in T_r$. Then $\begin{bmatrix}
l(i_1) \\
\vdots \\
l(i_m)
\end{bmatrix} = (-Q^{sup})^{-1}\begin{bmatrix}
V_{R_1}(i_1) \\
\vdots \\
V_{R_1}(i_m)
\end{bmatrix}$\\
\\where $Q^{sup}$ is the submatrix of Q corresponding to the transient states and\\
\\$V_{R_1} = \begin{bmatrix}
V_{R_1}(i_1) \\
\vdots \\
V_{R_1}(i_m)
\end{bmatrix} = \begin{bmatrix}
\sum_{j \in R_1} q(i_1,j)\\
\vdots \\
\sum_{j \in R_1} q(i_m,j)
\end{bmatrix}$.
\begin{example}
    A shop has two barbers that can cut hair at rate 3 people per hour. Customers arrive at times of a rate 2 Poisson process, but will leave if there are two people getting their haircut and two waiting. The state of the system to be the number of people in the shop. Find $P_i[T_0 < T_4]$ for $i = 1,2,3$.
    \\
    \\The idea is to treat {0} and {4} as absorbing states and following the below procedure:
    \begin{enumerate}
        \item Extract the matrix $Q^{sub}$\\
        \\$Q^{sub} = \begin{bmatrix}
         -5 & 2 & 0\\
         6 & -8 & 2\\
         0 & 6 & -8
        \end{bmatrix}$.
        \item Compute $(-Q^{sup})^{-1}$
        \item Compute $V_{R_1} = \begin{bmatrix}
         q(1,0)\\
         q(2,0)\\
         q(3,0)
        \end{bmatrix} = \begin{bmatrix}
         3\\
         0\\
         0
        \end{bmatrix}$.
        \item Compute $h = (-Q^{sup})^{-1}V_{R_1} = \begin{bmatrix}
         \frac{39}{41}\\
         \frac{36}{41}\\
         \frac{27}{41}
        \end{bmatrix}$.
        
    \end{enumerate}
    So, for instance, when having 1 initial customer, about 95\% of the time the shop will eventually be empty before being full.
\end{example}

\section{Final Remarks}

\section{Brownian Motion}

Brownian motion is a stochastic process, which is rooted in a physical phenomenon discovered almost 200 years ago. In 1827, the botanist Robert Brown, observing pollen grains suspended in water, noted the erratic and continuous movement of tiny particles ejected from the grains. He studied the phenomenon for many years, ruled out the belief that it emanated from some "life force" within the pollen, but could not explain the motion. Neither could any other scientist of the 19th century.
\\
\\In 1905, Albert Einstein solved the riddle in his paper, On the movement of small particles suspended in a stationary liquid demanded by the molecular-kinetic theory of heat. Einstein explained the movement by the continual bombardment of the immersed particles by the molecules in the liquid, resulting in "motions of such magnitude that these motions can easily be detected by a microscope". Einstein's theoretical explanation was confirmed 3 years later by empirical experiment, which led to the acceptance of the atomice nature of matter.

\subsection{Standard Brownian motion or Wiener Process}

\begin{definition}
    A continous-time process $\{B_t\}t\geq0$ is a standard Brownian motion (BM) if it satisfies the following properties:
    \begin{enumerate}
        \item $B_0 = 0$ (starts at time = 0)
        \item $B_{t+s}-B{s} \sim N(0,t)$, for any $t > 0, s \geq 0$ (\textbf{Stationary increments})
        \item $B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1}, \cdots, B_{t_n}-B_{t_{n-1}}$ are independent for any $0 \leq t_0 \leq t_1 \leq \cdots \leq t_n$ (\textbf{Independent increments})
        \item $t \mapsto B_t$ is a continuous finction of t (\textbf{Continuous paths})
    \end{enumerate}
\end{definition}

\subsection{Other closely related processes}

A process $\{X_t\}_t\geq0$ is a \textbf{BM with variance $\sigma ^2$} if it satisfies (1), (3) and (4) but (2) is replaced by $B_{t+s}-B{s} \sim N(0,\sigma ^2t)$, for any $t > 0, s \geq 0$.
\\
\\In addition, if (1) is replaced by $X_0 = x$, then we say that X is a \textbf{BM started at x with variance $\sigma^2$}. Note that $X_t = x + \sigma B_t$, where B is a standard BM.
\\
\\Unless otherwise stated, a BM refers to a standard Brownian motion.

\begin{remark}
    \begin{enumerate}
        \item $B_t \sim N(0,t) \Rightarrow E[B_t] = 0$ and $Var(B_t) = E[B_t^2] = t$
        \item $B_s$ and $B_{s+t} - B_{s}$ are independent for any $t, s \geq 0$.
        \item This definition is very similar to that of Poisson process but instead of a Poisson distribution, we have a Gaussian distribution.
    \end{enumerate}
\end{remark}

\begin{example}
    Find the covariance $Cov(B_s, B_t)$, $E[B_sB_t]$ and the correlation $Corr(B_s,B_t)$.
    \\
    \\
    \begin{equation*}
        \begin{split}
            E[B_sB_t] & = E[B_s(B_t-B_s+B_s)] \\
            & = E[B_s(B_t - B_s)] + E[B_s^2] \\
            & = E[B_s]E[B_t - B_s] + Var(B_s) \\
            & = s
        \end{split}
    \end{equation*}
    The third equality follows by the independent increment and the last equality is because $E[B_s] = 0$ and $Var(B_s) = s$.
    
\end{example}

\subsection{Properties of conditional Brownian motion}

\begin{enumerate}
    \item $E[B_t|B_s] = E[B_t-B_s+B_s|B_s] = E[B_t-B_s|B_s] + E[B_s|B_s] = E[B_t-B_s] + B_s = 0 + B_s = B_s$
    \item We want $E[B_t|B_s]$ and $Var(B_t|B_s)$. Note that $B_t$ and $B_s - \frac{s}{t}B_t$ are independent. Since $Cov(B_t, B_s - \frac{s}{t}B_t) = Cov(B_t, B_s) - \frac{s}{t}Cov(B_s,B_t) = s - \frac{s}{t}\times t = 0$
\end{enumerate}

\subsection{Markov property of BM}

\begin{theorem}
    BM is a markov process
    \\
    \\
    \begin{math}
    P(B_{s+t}\leq y | B_s = x) = P(B_{s+t} - B_s \leq y - x | B_s = x)
    =P(B_{s+t} - B_s \leq y - x)
    =\Phi (\frac{y-x}{\sqrt{t}})
    \end{math}
    \\
    \\It follows that $B_{s+t}|B_s \sim N(B_s,t)$
\end{theorem}

\begin{theorem}
    For any fixed time $s \in (0, \infty)$, TFAE
    \begin{enumerate}
        \item $B_{t+s} | B_s \sim N(B_s, t)$
        \item $B_{t+s} - B_s | B_s \sim N(0,t)$
        \item $\{B_{t+s}\}_{t\geq 0}$ is a BM independent of $\{B_u\}_{u \leq s}$
    \end{enumerate}
\end{theorem}

\subsection{Strong Markov Property and its implication}

Let $a \in \mathbb{R}$ and $T_a = min\{t > 0 | B_t = a\}$. Then $\{B_{t+T_a} - B_{T_a}\}_{t\geq 0}$ is a BM independent of $\{B_u\}_{u\leq 0}$.

\begin{enumerate}
    \item $P(T_a \leq x)$
\end{enumerate}

\end{document}
